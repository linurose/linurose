# -*- coding: utf-8 -*-
"""tcs_internship_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pb1r74HpLv6ArUCQ2vvnfTERnEY-nCJ8

# **Build a Python application to rank and classify Smartphone features.**
"""

#importing basic libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#reading the CSV file to python environment train dataset
mobile_data=pd.read_csv("/content/MobileTrain.csv")

#reading the CSV file to python environment test dataset
mobile_data_test=pd.read_csv("/content/MobileTest.csv")

#checking whether the dataset is correctly loaded by displaying the 1st 3 and last 3 columns
print("The first three entries of the dataset is :\n")
mobile_data.head(3)

print("\nThe last three rows of the dataset is :\n ")
mobile_data.tail(3)

"""Thus we can be sure that all the 2000(0 to 1999) rows of data's are loaded.

**Understanding the dataset.**
"""

#checking the size of the dataset
print("The size of train dataset: " , mobile_data.shape)
print("The size of test dataset: " , mobile_data_test.shape)

"""Our train dataset contains 2000 row enteries and 21 column enteries and test data contains 1000 rows and 21 columns"""

#checking the column headers
print("The column headers of train dataset are : " , mobile_data.columns)

print("\n The column headers of test dataset are : " , mobile_data_test.columns)

"""Our dataset contains columns like :
  ['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',
       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',
       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',
       'touch_screen', 'wifi', 'price_range'] which contains basic features of a smartphone.
       The test data doesnt contain our target column price range, where as train data contains it so its a suoervised learning method. The additional column that test data contains is the id column, which we can omit.
"""

#checking the data types of each column
print("The data types of train data column are : \n")
mobile_data.dtypes

"""No categorical datas are present so encoding is not needed."""

#checking basic info of a dataset
print("The data type and number of non null values in train dataset : \n")
mobile_data.info()

"""It gives us the column header name, no:of non null values and data types of each columns of our dataset"""

print("The basic description of train dataset is : \n")
mobile_data.describe()

"""From this we can see the count, mean , standard deviation, minimum and maximum values and the three quartile values of all the columns present in the dataset.
Here we can see the standard deviation is very large for battery power This shows how the data's are spread from the mean. A low standard deviation indicates that the data points tend to be very close to the mean; a high standard deviation indicates that the data points are spread out over a large range of values. So its good to apply scaling to the dataset.
"""

#checking for null valuues
print("The null values contains in each solumn of train dataset is as : \n")
print(mobile_data.isnull().sum())
print("\n The null values contains in each solumn of test dataset is as : \n")
print(mobile_data_test.isnull().sum())

"""This shows there no null values in the dataset, so the step as cleaning the dataset is not needed to be done.

**Now we can move to the next step to create some meaningful insights from the dataset. Visualizing the data will be more meaningful that seeing the numbers.**

**Data Visualization/ Exploratory data analysis(EDA)**

*1. Univariant Analysis*
"""

#countplot
column_name=mobile_data.columns
len_c=len(column_name)
j=1
plt.figure(figsize=(30,30))
for i in range(0,len_c):
  plt.subplot(5,5,j)
  sns.countplot(x=mobile_data[column_name[i]])
  j=j+1
plt.show()

"""its not easy to interpret from this count plot, so what we are going to dois that we are going to delete columns that have more no:of unique values and also going to plot count plots as two seperate plots, one which contain unique value below 10 and other with unique values between 10 and 30. deleting all other columns above 30 unique enteries, since we cant get a meaningful visualization from that."""

#checking for the unique no:of enteries in the dataframe
print("\n The unique values in each column are: \n")
print(mobile_data.nunique())

#mobile_data.shape

#creating a copy of the dataset which contains columns which have unique value less than 10.
mobile_data_copy=mobile_data[["blue","dual_sim","four_g","n_cores","three_g","touch_screen","wifi","price_range"]]
print("The shape of the new dataframe : ", mobile_data_copy.shape)
#mobile_data.shape

#count plot for the new (less than 10) dataframe
column_name=mobile_data_copy.columns
len_c=len(column_name)
print("Length of the new dataset:",len_c)
j=1
plt.figure(figsize=(20,20))
for i in range(0,len_c):
  print(mobile_data_copy[column_name[i]].value_counts())
  plt.subplot(5,3,j)
  sns.countplot(x=mobile_data_copy[column_name[i]])
  plt.title(column_name[i])
  j=j+1
plt.tight_layout()  
plt.show()

"""From the graph and values we got we gets a clear picture of the dataset.
 For example.
 1. In case of the column bluetooth, 1010 doesnt have bluetooth and 990 have bluetooth in it.
 2. Similarly, in case of dual sim specification, 1019 have one and 981 doesnt. These can be seen from the graph too.
 3. from the graph its clear that for 3G facility, most of the phone have one.
 4. tHus we can intrepret for all the columns.



"""

#mobile_data.shape

# taking only columns that have unique value above 10 and below 30 and creating a new dataframe:"mobile_data_copy_2"
mobile_data_copy_2=mobile_data.drop(["blue","dual_sim","four_g","n_cores","three_g","touch_screen","wifi","price_range","battery_power","int_memory","mobile_wt","px_height","px_width","ram"], axis=1)
print(mobile_data_copy_2.shape)

#count plot
column_name=mobile_data_copy_2.columns
len_c=len(column_name)
print("Length of the new dataset:",len_c)
j=1
plt.figure(figsize=(30,30))
for i in range(0,len_c):
  print(mobile_data_copy_2[column_name[i]].value_counts().sort_values(ascending=False))
  plt.subplot(5,3,j)
  sns.countplot(x=mobile_data_copy_2[column_name[i]])
  plt.title(column_name[i])
  j=j+1
plt.show()

"""From the above facts and figures, we can see the unique count for most of the hardware part of the dataset.


"""

#dealing with continous variable in the dataset
#Seaborn Distplot represents the overall distribution of continuous data variables.
#distplot
mobile_data_copy_3=mobile_data[["battery_power","int_memory","mobile_wt","px_height","px_width","ram","clock_speed","m_dep","talk_time"]]
print(mobile_data_copy_3.shape)

column_name=mobile_data_copy_3.columns
len_c=len(column_name)
print("Length of the new dataset:",len_c)
j=1
plt.figure(figsize=(20,20))

for i in range(0,len_c):
  plt.subplot(3,3,j)
  sns.distplot(x=mobile_data_copy_3[column_name[i]],kde=True)
  j=j+1
  
plt.show()

"""Now we can see the distribution of continous variables of our dataframe.

"""

#pie chart
mobile_data_copy.head(2)

#pie chart of devices having bluetooth facility
mobile_data_copy['blue'].value_counts().plot(kind='pie', y='blue', autopct='%1.3f%%',title="Pie chart showing the percentage of devices having bluetooth",colors = ['orange', 'pink'])

#pie chart of devices having dual_sim facility
mobile_data_copy['dual_sim'].value_counts().plot(kind='pie', y='dual_sim', autopct='%1.3f%%',title="Pie chart showing the percentage of devices having dual_sim",colors = ['cyan', 'yellow'])

#pie chart of devices having 4G facility
mobile_data_copy['four_g'].value_counts().plot(kind='pie', y='four_g', autopct='%1.3f%%',title="Pie chart showing the percentage of devices having 4G",colors = ['orange', 'yellow'])

#pie chart of devices depending on the cores
colors = ( "orange", "cyan", "lightblue", "grey","pink","blue","white","red")
explode = (0.1, 0, 0,0,0,0,0,0)
mobile_data_copy['n_cores'].value_counts().plot(kind='pie', y='n_cores', autopct='%1.3f%%',explode=explode,title="Pie chart showing the percentage of devices depending on the core of the processor", colors = colors)

#pie chart of devices having three g facility
mobile_data_copy['three_g'].value_counts().plot(kind='pie', y='three_g', autopct='%1.3f%%',explode=(0.2,0),title="Pie chart showing the percentage of devices having three G facility",colors = ['cyan', 'pink'])

#pie chart of devices having touch screen facility
mobile_data_copy['touch_screen'].value_counts().plot(kind='pie', y='touch_scree', autopct='%1.3f%%',title="Pie chart showing the percentage of devices having touch screen",colors = ['pink', 'blue'])

#pie chart of devices having wifi facility
mobile_data_copy['wifi'].value_counts().plot(kind='pie', y='wifi', autopct='%1.3f%%',title="Pie chart showing the percentage of devices having wi fi",colors = ['orange', 'pink'])

#pie chart of devices depending on the price range
mobile_data_copy['price_range'].value_counts().plot(kind='pie', y='price_range', autopct='%1.3f%%',title="Pie chart showing the percentage of price range of device available",colors = ['yellow', 'white'])

"""*Bivariant analysis*"""

#price dependence on no:of cores
sns.countplot(data=mobile_data,x="price_range",hue="n_cores")
plt.title("shows the price range with no:of cores in the processor")
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.show()

# price range dependence on different features using count plot
plt.figure(figsize=(15,8))
plt.subplot(2,3,1)
sns.countplot(data=mobile_data,x="three_g",hue="price_range")
plt.title("Price range dependence on three g facility availability")


plt.subplot(2,3,2)
sns.countplot(data=mobile_data,x="four_g",hue="price_range",palette="Set2")
plt.title("Price range dependence on four g facility availability")
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

plt.subplot(2,3,3)
sns.countplot(data=mobile_data,x="dual_sim",hue="price_range",palette="Set3")
plt.title("Price range dependence on dual sim facility availability")
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

plt.subplot(2,3,4)
sns.countplot(data=mobile_data,x="wifi",hue="price_range",palette=["red","orange","pink","yellow"])
plt.title("Price range dependence on wi fi facility availability")
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

plt.subplot(2,3,5)
sns.countplot(data=mobile_data,x="touch_screen",hue="price_range",palette=["blue","green","violet","cyan"])
plt.title("Price range dependence on touch screen facility availability")
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

plt.tight_layout()
plt.show()

#dependence of screen height and width

sns.lineplot(x="sc_h",y="sc_w",data=mobile_data)

#front camera and primary camera dependence
sns.lineplot(x="pc",y="fc",data=mobile_data)

#pairplot
sns.pairplot(mobile_data)
plt.title("The pair plot of the train dataset")
plt.show()

"""The above graph shows the pairplot ie; each feature of the dataset is plotted with respect to each other"""

#histogram of the train dataset
mobile_data.hist(figsize=(30,20))
plt.title("Histogram of all features in train dataset")
plt.show()

#swarmplot of price range with ram and battery power
print("Since ram and battery power havehigh correlation with price range we are going to plot the swarmplot of those")
plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
sns.swarmplot(x = "price_range",y = "ram",data =mobile_data)
plt.title("Swarmplot of price range with ram")
plt.subplot(1,2,2)
sns.swarmplot(x = "price_range",y = "battery_power",data =mobile_data)
plt.title("Swarmplot of price range with battery power")
plt.tight_layout()
plt.show()

"""For high price range RAM features is high. So as price range increases, ram faeture is also increasing.
Battery power and price range have no much dependence.

**Splitting into hardware and software components**

For high price range RAM features is high. So as price range increases, ram faeture is also increasing.
Battery power and price range have no much dependence.

This is done by treating n_cores and clock speed as software components and rest as hardware
"""

hardware_comp= mobile_data.loc[:,['battery_power','blue','dual_sim','fc','four_g','int_memory','m_dep','mobile_wt','n_cores','pc','px_height','px_width','ram','sc_h','sc_w','talk_time','three_g','wifi','price_range']]
hardware_comp.head(3)

software_comp= mobile_data.loc[:,['n_cores','clock_speed']]
software_comp.head(2)

"""**Feature Reduction**"""

#correlation
print("Correaltion heatt map of train data")
data_corr=mobile_data.corr()
plt.figure(figsize=(20,10))
sns.heatmap(data_corr,annot=True)
plt.show()

print("Correaltion heatt map of test data")
data_corr_test=mobile_data_test.corr()
plt.figure(figsize=(20,10))
sns.heatmap(data_corr_test,annot=True)
plt.show()

correlation = data_corr['price_range'].sort_values(ascending = False)
corr = correlation[1:]
sns.barplot(corr,corr.index)
plt.title("Correlation of other feature with price range")
plt.tight_layout()
plt.show()

"""Only price range and RAM have high correlation, we cant delete any of those columns because price range is our target variable and price range have high dependence on RAM sprecification. So we are not deleting any features. """



"""**Outlier Handling** """

mobile_data.columns

#combining test and train data to remove outliers
mobile_data_new = pd.concat([mobile_data_test.assign(ind="test"), mobile_data.assign(ind="train")])
print("The shape of final dataset is :  ", mobile_data_new.shape)

#outlier handling
fig, axes = plt.subplots(2, 3, figsize=(8,8))
sns.boxplot(  y='px_width', data=mobile_data_new,  orient='v' , ax=axes[0, 0])
sns.boxplot(  y='ram',  data=mobile_data_new,  orient='v' , ax=axes[0, 1])
sns.boxplot(  y='sc_h', data=mobile_data_new,  orient='v' , ax=axes[0, 2])
sns.boxplot(  y='sc_w', data=mobile_data_new,  orient='v' , ax=axes[1, 0])
sns.boxplot(  y='talk_time',  data=mobile_data_new,  orient='v' , ax=axes[1, 1])
sns.boxplot(  y='wifi',  data=mobile_data_new,  orient='v' , ax=axes[1, 2])
plt.tight_layout()
plt.show()

"""No outlier in these columns"""

#outlier handling
fig, axes = plt.subplots(2, 3, figsize=(8,8))
sns.boxplot(  y='int_memory', data=mobile_data_new,  orient='v' , ax=axes[0, 0])
sns.boxplot(  y='m_dep',  data=mobile_data_new,  orient='v' , ax=axes[0, 1])
sns.boxplot(  y='mobile_wt', data=mobile_data_new,  orient='v' , ax=axes[0, 2])
sns.boxplot(  y='n_cores', data=mobile_data_new,  orient='v' , ax=axes[1, 0])
sns.boxplot(  y='pc',  data=mobile_data_new,  orient='v' , ax=axes[1, 1])
sns.boxplot(  y='px_height',  data=mobile_data_new,  orient='v' , ax=axes[1, 2])
plt.tight_layout()
plt.show()

"""No outliers"""

#outlier handling
fig, axes = plt.subplots(2, 3, figsize=(8,8))
sns.boxplot(  y='battery_power', data=mobile_data_new,  orient='v' , ax=axes[0, 0])
sns.boxplot(  y='blue',  data=mobile_data_new,  orient='v' , ax=axes[0, 1])
sns.boxplot(  y='clock_speed', data=mobile_data_new,  orient='v' , ax=axes[0, 2])
sns.boxplot(  y='dual_sim', data=mobile_data_new,  orient='v' , ax=axes[1, 0])
sns.boxplot(  y='fc',  data=mobile_data_new,  orient='v' , ax=axes[1, 1])
sns.boxplot(  y='four_g',  data=mobile_data_new,  orient='v' , ax=axes[1, 2])
plt.tight_layout()
plt.show()

"""outlier is there only for the column fc so dropping those columns"""

Q1=np.percentile(mobile_data_new["fc"],25,interpolation="midpoint")
print("Q1)",Q1)
Q2=np.percentile(mobile_data_new["fc"],50)
print("Q2",Q2)
Q3=np.percentile(mobile_data_new["fc"],75)
print("Q3",Q3)

IQR=Q3-Q1
print("IQR",IQR)

low_lim=Q1-1.5*IQR
upp_lim=Q3+1.5*IQR

print("low_lim",low_lim)
print("upp_lim",upp_lim)

outlier=[]
for x in mobile_data_new["fc"]:
    if (x>upp_lim) or (x<low_lim):
        outlier.append(x)

print("outlier", outlier)

ind1=mobile_data_new["fc"]>upp_lim

ind_1=mobile_data_new.loc[ind1].index

#print("index of outlier", ind1)

mobile_data_new.drop(ind_1,inplace=True)

sns.boxplot(  y='fc', data=mobile_data_new)
plt.title("Box plot of fc column after deleting outliers")
plt.show()

"""now no ouliers left in the dataset

**Seperating the two datasets back to train and test sets**
"""

mobile_data_test, mobile_data = mobile_data_new[mobile_data_new["ind"].eq("test")], mobile_data_new[mobile_data_new["ind"].eq("train")]

print("The train set details:", mobile_data.shape)
mobile_data=mobile_data.drop(['id','ind'],axis=1)
print("\n", mobile_data.columns)

print("The test set details:", mobile_data_test.shape)
mobile_data_test=mobile_data_test.drop(['id','ind'],axis=1)
print("\n", mobile_data_test.columns)

"""**Splitting the dataset into independent and dependent variables**"""

#independent variable "x_data"
x_data=mobile_data.drop(["price_range"],axis=1)
#dependent variable "y_data"
y_data=mobile_data["price_range"]
print(x_data.shape)
print(y_data.shape)
x_data_1=x_data.copy()
x_data_1.head(2)

"""**Scaling**"""

#standard scaling
from sklearn.preprocessing import StandardScaler
scalar=StandardScaler()
x_data=scalar.fit_transform(x_data)
x_data=pd.DataFrame(x_data,columns=x_data_1.columns)
x_data.head(3)

"""**MODEL BUILDING**

Train test split
"""

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x_data,y_data,random_state=0,test_size=0.25)

print(x_train.shape)
print(y_train.shape)

print(x_test.shape)
print(y_test.shape)

"""Since it is a classification problem we are going to use different classification algorithms for our dataset. The main used here are :


1.   Logistic Regression
2.   k Nearest Neighbors
3.   Decision Tree
4.   Random Forest
5.   SVM
6.   Xg booster
7.   CatBooster
8.`  Naive Bayes



"""

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from xgboost import XGBClassifier

from sklearn.metrics import confusion_matrix, accuracy_score, f1_score,precision_score,recall_score,classification_report

"""**1. Logistic Regression**"""

#creating an instance of logistic regression
lr = LogisticRegression()
#fitting the model
lr.fit(x_train,y_train)
#predicting the model
y_pred_lr=lr.predict(x_test)

#evaluation metrics
#training score
train_lr=round(lr.score(x_train,y_train),3)
print("the training score of logistic regression is : ", round(lr.score(x_train,y_train),3))
#testing score
acc_lr=accuracy_score(y_test,y_pred_lr)
print("Test Accuracy of Logistic Regression is : ",round(acc_lr,4)*100,"%" )
#confusion matrix
con_lr=confusion_matrix(y_test,y_pred_lr)
print("\n The confusion matrix of logistic regression is \n\n",con_lr)
#classification report
print("\n The classification report of logistic regression is : \n")
print(classification_report(y_test, y_pred_lr))

plt.figure(figsize=(8,5))
sns.heatmap(con_lr, annot=True)
plt.title("Confusion matrix of Logistic Regression")
plt.show()

"""**2. K Nearest Neighbors**"""

acc_values=[]
import numpy as np
neighbor_1= np.arange(3,20)

#finding the optimum k number
for k in neighbor_1:
    classifier=KNeighborsClassifier(n_neighbors=k,metric="minkowski")
    classifier.fit(x_train,y_train)
    y_pred_knn=classifier.predict(x_test)
    acc=accuracy_score(y_test,y_pred_knn)
    acc_values.append(round(acc,2))
    
print("The Accuracy score for k values = 3 to 20 is s : \n ",acc_values)

#plotting the k Vs Accuracy value curve
plt.plot(neighbor_1,acc_values)
plt.xlabel("K values----")
plt.ylabel("Accuracy")
plt.title("No:of neighbor Vs Accuracy")

#for k = 15 we got high accuracy
#K Nearest Neighbor Classifier instance creation
knn=KNeighborsClassifier(n_neighbors=15,metric="minkowski")
#fitting the model
knn.fit(x_train,y_train)
#predicting the model
y_pred_knn=knn.predict(x_test)


#Accuracy score of training set
train_knn=round(knn.score(x_train,y_train),3)
print("the training score of KNN is : ", round(knn.score(x_train,y_train),3))
#test accuracy calculation
acc_knn=accuracy_score(y_test,y_pred_knn)
print("The test accuracy score of KNN is :",round(acc_knn,4)*100,"%")
#confusion matrix
con_knn=confusion_matrix(y_test,y_pred_knn)
print("\n  Confusion matrix of KNN is : \n",con_knn)
print("\n The classification report of KNN :")
print(classification_report(y_test, y_pred_knn))

plt.figure(figsize=(8,5))
sns.heatmap(con_knn, annot=True,cmap="YlGnBu")
plt.title("Confusion matrix of KNN")
plt.show()

"""**3. Decision Tree**"""

dt = DecisionTreeClassifier()
#fitting the model
dt.fit(x_train,y_train)
#predicting the classifier
y_pred_dt=dt.predict(x_test)


#The accuarcy score of training set
train_dt=round(dt.score(x_train,y_train),3)
print("the training score of decision treee classifier is : ", round(dt.score(x_train,y_train),3))
#test set accuracy calculation
acc_dt=accuracy_score(y_test,y_pred_dt)
print("Test Accuracy of decision tree classifier is: ", round(acc_dt,2)*100,"%")
#confusion matrix
con_dt=confusion_matrix(y_test,y_pred_dt)
print("\n Confusion matrix of decision tree classifier is : \n",con_dt)
#classification report
print("\n The classification report of Decision Tree classifier:")
print(classification_report(y_test, y_pred_dt))

plt.figure(figsize=(8,5))
sns.heatmap(con_dt, annot=True,cmap="Greens")
plt.title("Confusion matrix of decision tree")
plt.show()

"""**4. Random Forest Classifier**"""

rf = RandomForestClassifier( n_estimators = 500)
#fitting the model
rf.fit(x_train,y_train)
#predicting the model
y_pred_rf=rf.predict(x_test)

#evaluation metrics
#The accuarcy score of training set
train_rf=round(rf.score(x_train,y_train),3)
print("the training score of decision treee classifier is : ", round(rf.score(x_train,y_train),3))
#test accuracy
acc_rf=accuracy_score(y_test,y_pred_rf)
print("Test Accuracy of Random Forest classifier is: ", round(acc_rf,4)*100,"%")
#confusion matrix
con_rf=confusion_matrix(y_test,y_pred_rf)
print("\n Confusion matrix of Random forest classifier is : \n",con_rf)
#classification report
print("\nThe classification report of Random Forest classifier is:")
print(classification_report(y_test, y_pred_rf))

plt.figure(figsize=(8,5))
sns.heatmap(con_rf, annot=True,cmap="BuPu")
plt.title("Confusion matrix of Random Forest classifier")
plt.show()

"""**5. Naive Bayes**"""

nb = GaussianNB()
#fitting
nb.fit(x_train, y_train)
#predicting
y_pred_nb = nb.predict(x_test)

#The accuarcy score of training set
train_nb=round(nb.score(x_train,y_train),3)
print("the training score of Naive Bayes classifier is : ", round(nb.score(x_train,y_train),2)*100,"%")
from sklearn.metrics import accuracy_score
#test accuracy
acc_nb=accuracy_score(y_test, y_pred_nb)
print("The test accuracy of Naive bayes is : ", round(acc_nb*100,2),"%")
from sklearn.metrics import confusion_matrix
print("The confusion matrix of Naive Bayes is :\n", confusion_matrix(y_test, y_pred_nb))
#classification report
print("\nThe classification report of Naive Bayes classifier is:")
print(classification_report(y_test, y_pred_nb))

con_nb=confusion_matrix(y_test, y_pred_nb)
plt.figure(figsize=(8,5))
sns.heatmap(con_nb, annot=True,cmap="Blues")
plt.title("Confusion matrix of Naive Bayes classifierr")
plt.show()

""" **6.Support Vector Machine**"""

svc=SVC(kernel="rbf")
#fitting
svc.fit(x_train,y_train)
#predicting
y_pred_svm=svc.predict(x_test)

#training score
train_svm=round(svc.score(x_train,y_train),3)
print("Training score of SVM is : ", round(svc.score(x_train,y_train),3))
#accuracy score
from sklearn.metrics import accuracy_score
acc_svm=accuracy_score(y_test,y_pred_svm)
print("The test accuracyscore of SVM is :, ", round(acc_svm,3)*100,"%")
#confusion matrix
print("The confusion matrix of SVM is :\n", confusion_matrix(y_test, y_pred_svm))
#classification report
print("\nThe classification report of SVM is:")
print(classification_report(y_test, y_pred_svm))

con_svm=confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(8,5))
sns.heatmap(con_svm, annot=True,cmap="PiYG")
plt.title("Confusion matrix of SVM")
plt.show()

"""**7. XG Booster**"""

xgb = XGBClassifier(random_state = 0, learning_rate = 0.1)

xgb.fit(x_train, y_train)

y_pred_xgb = xgb.predict(x_test)

#training score
train_xgb=round(xgb.score(x_train,y_train),3)
print("Training score of xgb classifier is : ", round(xgb.score(x_train,y_train),3))
acc_xgb = accuracy_score(y_test, y_pred_xgb)
print("Accuracy score of extreme gradient boosting is", round(acc_xgb*100,2),"%")
#confusion matrix
print("\n\nThe confusion matrix of extreme gradient boosting is :\n", confusion_matrix(y_test, y_pred_xgb))
#classification report
print("\nThe classification report of extreme gradient boosting is:")
print(classification_report(y_test, y_pred_xgb))

con_xgb=confusion_matrix(y_test, y_pred_xgb)
plt.figure(figsize=(8,5))
sns.heatmap(con_xgb, annot=True,cmap="coolwarm")
plt.title("Confusion matrix of Extreme Gradient Boosting")
plt.show()

"""**6. Cat Boost**"""

pip install catboost

from catboost import CatBoostClassifier

cat = CatBoostClassifier()

cat.fit(x_train, y_train, plot=True, early_stopping_rounds=30, verbose=100)

y_pred_cat=cat.predict(x_test)

y_pred_cat=cat.predict(x_test)

from sklearn.metrics import confusion_matrix, accuracy_score, f1_score,precision_score,recall_score,classification_report


#training score
train_cat=round(cat.score(x_train,y_train),3)
print("Training score of catboost classifier is : ", round(cat.score(x_train,y_train),3))
acc_cat=accuracy_score(y_test,y_pred_cat)
print("Test Accuracy of Cat Boost is : ",round(acc_cat,3)*100,"%" )

con_cat=confusion_matrix(y_test,y_pred_cat)
print("\n The confusion matrix of Cat Boost is \n\n",con_cat)

#classification report
print("\nThe classification report of category booster is:")
print(classification_report(y_test, y_pred_cat))

con_cat=confusion_matrix(y_test, y_pred_cat)
plt.figure(figsize=(8,5))
sns.heatmap(con_cat, annot=True)
plt.title("Confusion matrix of Ecat boost classifier")
plt.show()

Results = pd.DataFrame(columns = ['Model', 'Training Score',"Accuracy Score"])
Results = Results.append({'Model' : 'Random Forest', 'Training Score': train_rf,"Accuracy Score":round(acc_rf,2)}, ignore_index = True)
Results = Results.append({'Model' : 'Logistic Regression', 'Training Score': train_lr,"Accuracy Score":round(acc_lr,2)}, ignore_index = True)
Results = Results.append({'Model' : 'KNN', 'Training Score': train_knn,"Accuracy Score":round(acc_knn,2)}, ignore_index = True)
Results = Results.append({'Model' : 'Decision Treet', 'Training Score': train_dt,"Accuracy Score":round(acc_dt,2)}, ignore_index = True)
Results = Results.append({'Model' : 'XGBoost', 'Training Score': train_xgb,"Accuracy Score":round(acc_xgb,2)}, ignore_index = True)
Results = Results.append({'Model' : 'CatBoost', 'Training Score': train_cat,"Accuracy Score":round(acc_cat,2)}, ignore_index = True)
Results = Results.append({'Model' : 'Naive Bayes', 'Training Score': train_nb,"Accuracy Score":round(acc_nb,2)}, ignore_index = True)
Results = Results.append({'Model' : 'SVM', 'Training Score': train_svm,"Accuracy Score":round(acc_svm,2)}, ignore_index = True)

Results.sort_values("Accuracy Score",ascending=False)
Results.reset_index(drop=True, inplace=True)

print("The table that contains the training score and accuracy(test) score for different models")
Results.sort_values("Accuracy Score",ascending=False)

"""The above table gives the accuracy score and training score for various classification models

**Logistic regression have the highest accuracy score.**

**Hyper parameter Tuning**

Now we are trying to improve the accuracy of catbooster classifier, extreme gradient boosting classifier and Support vector machine using hyper parameter tuning. We are going to change the hyperparameters of these three models using GridSearchCV method.

1. Hyper Parameter Tuning **Random Forest Classifier**
"""

from sklearn.model_selection import GridSearchCV
rf=RandomForestClassifier(random_state=0)
param_grid = { 
    'n_estimators': [10,100, 200, 500,1000],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [4,5,6,7,8,10,20],
    'criterion' :['gini', 'entropy'],
    'min_samples_split':[2,4,6,8]
}

CV_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 5)
CV_rf.fit(x_train, y_train)

print(CV_rf.best_params_)

rf_1=RandomForestClassifier(random_state=0,  min_samples_split= 6, max_features='auto', n_estimators=200 , max_depth=20, criterion='entropy')
rf_1.fit(x_train, y_train)
y_pred=rf_1.predict(x_test)
acc_rf_new= round(accuracy_score(y_test,y_pred),2)
print("Accuracy for Random Forest on CV data: ",acc_rf_new)

print('Improvement of {:0.2f}% in accuracy after performing hyper parameter tuning. '.format( 100 * (acc_rf_new - acc_rf) / acc_rf))

"""2. **Hyper parameter tuning forxg booster**"""

# A parameter grid for XGBoost
params = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5]
        }

xgb = XGBClassifier()
CV_xgb = GridSearchCV(estimator=xgb, param_grid=params, cv= 5)
CV_xgb.fit(x_train, y_train)

print(CV_xgb.best_params_)

xgb_1=XGBClassifier(random_state = 0, learning_rate = 0.1, max_depth=5,gamma=0.5,subsample=1.0,colsample_bytree=1.0,min_child_weight=1)
xgb_1.fit(x_train,y_train)
y_pred_x=xgb_1.predict(x_test)
acc_xgb_new=round(accuracy_score(y_test,y_pred_x),3)
print("Accuracy for xb boost on CV data: ",accuracy_score(y_test,y_pred_x))

print('Improvement of {:0.2f}% in accuracy after performing hyper parameter tuning. '.format( 100 * (acc_xgb_new - acc_xgb) / acc_xgb))

"""** 3. Hyper parameter tuning for SVM**"""

# defining parameter range
param_grid = {'C': [0.1, 1, 10, 100, 1000],
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']}
 
grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)

grid.fit(x_train, y_train)

# print best parameter after tuning
print("\n\nThe Best parameters are: \n")
print(grid.best_params_)

svc=SVC(kernel="rbf",C= 1000, gamma= 0.0001)
svc.fit(x_train,y_train)
y_pred_svm_new=svc.predict(x_test)

#accuracy score

acc_svm_new=accuracy_score(y_test,y_pred_svm_new)
print("The test accuracy score of SVM after grid search cv is :, ", round(acc_svm_new,3))

print('Improvement of {:0.2f}% in accuracy after performing hyper parameter tuning. '.format( 100 * (acc_svm_new - acc_svm) / acc_svm))

"""4.** Hyper parameter tuning for CAT Booster**"""

from sklearn.metrics import make_scorer, accuracy_score
clf = CatBoostClassifier()
params = {'iterations': [100.200,500,1000],
          'depth': [4, 5, 6.8,10],
          'loss_function': ["MultiClass","MultiClassOneVsAll"],
          'leaf_estimation_iterations': [5,10,15],
          'logging_level':['Silent'],
          'random_seed': [0]
         }
scorer = make_scorer(accuracy_score)
clf_grid = GridSearchCV(estimator=clf, param_grid=params, scoring=scorer, cv=5)



clf_grid.fit(x_train, y_train)
best_param = clf_grid.best_params_
print("The best parameter for cat boost is : ")
print(best_param)

cat_b = CatBoostClassifier(iterations=1000,
                           loss_function='MultiClass',
                           depth=5,
                           leaf_estimation_iterations=5,
                           logging_level='Silent',
                           random_seed=0
                          )

cat_b.fit(x_train,y_train)
y_pred_cat=cat_b.predict(x_test)
acc_cat_new=accuracy_score(y_test,y_pred_cat)
print("Accuracy for cat boost on CV data: ",accuracy_score(y_test,y_pred_cat))

print('Improvement of {:0.2f}% in accuracy after performing hyper parameter tuning. '.format( 100 * (acc_cat_new - acc_cat) / acc_cat))

"""**5.Hyper parameter tuning for Logistic Regression**"""

from sklearn.model_selection import RepeatedStratifiedKFold

solvers = ['newton-cg', 'lbfgs', 'liblinear']
penalty = ['l2']
c_values = [100, 10, 1.0, 0.1, 0.01]
grid = dict(solver=solvers,penalty=penalty,C=c_values)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)
grid_search = GridSearchCV(estimator=lr, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
grid_result = grid_search.fit(x_train, y_train)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

lr=LogisticRegression(C= 10, penalty = 'l2', solver = 'newton-cg')

lr.fit(x_train,y_train)
y_pred_lr=lr.predict(x_test)
acc_lr_new=accuracy_score(y_test,y_pred_lr)
print("Accuracy for Logistic Regression on CV data: ",accuracy_score(y_test,y_pred_lr))

print('Improvement of {:0.2f}% in accuracy after performing hyper parameter tuning. '.format( 100 * (acc_lr_new - acc_lr) / acc_lr))

""" **6.Hyper parameter tuning for KNN**"""

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

n_neighbors = range(1, 22, 2)
weights = ['uniform', 'distance']
metric = ['euclidean', 'manhattan', 'minkowski']
# define grid search
grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)
cvalidation = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=knn, param_grid=grid, n_jobs=-1, cv=cvalidation, scoring='accuracy',error_score=0)
grid_result = grid_search.fit(x_train, y_train)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

"""**Dataframe showing the model name, the accuracy score before and after hyper parameter tuning**"""

print("The table showing the accuracy score of various models before and after hyper parameter tuning")
Result_new = pd.DataFrame(columns = ['Model', 'Initial Accuracy Score',"Accuracy Score after hyper parameter tuning"])
Result_new = Result_new.append({'Model' : 'Random Forest', 'Initial Accuracy Score': acc_rf,"Accuracy Score after hyper parameter tuning":acc_rf_new}, ignore_index = True)
Result_new = Result_new.append({'Model' : 'Extreme Gradient Boosting', 'Initial Accuracy Score': acc_xgb,"Accuracy Score after hyper parameter tuning":acc_xgb_new}, ignore_index = True)
Result_new = Result_new.append({'Model' : 'Suport Vector Machine', 'Initial Accuracy Score': acc_svm,"Accuracy Score after hyper parameter tuning":acc_svm_new}, ignore_index = True)
Result_new = Result_new.append({'Model' : 'Category Booster', 'Initial Accuracy Score': acc_cat,"Accuracy Score after hyper parameter tuning":acc_cat_new}, ignore_index = True)
Result_new = Result_new.append({'Model' : 'Logistic Regression', 'Initial Accuracy Score': acc_lr,"Accuracy Score after hyper parameter tuning":acc_lr_new}, ignore_index = True)
Result_new.sort_values("Accuracy Score after hyper parameter tuning",ascending=False).reset_index(drop=True)

print("Here we can see that Logistic regression, CAt boost classifier and SVM have good accuracy")

"""**Predicting the test data outputs**

**Now we are going to predict the price range for test data CSV using CAT boost classifier algorithm**
"""

train_data=mobile_data.copy()

pip install catboost

from catboost import CatBoostClassifier

cat = CatBoostClassifier()
train_data_x=train_data.iloc[:,0:20]
train_data_y=train_data["price_range"]

cat.fit(train_data_x,train_data_y, plot=True, early_stopping_rounds=30, verbose=100)

y_pred_test=cat.predict(mobile_data_test)

mobile_data_test["price_range"]=y_pred_test
mobile_data_test.head()

"""Now the price range of test dataset is predicted, now we are going to create a combined dataset.

**Merging  test and train dataframes.**
"""

new_mobile_data=pd.concat([train_data,mobile_data_test],axis=0)
new_mobile_data.head()

print("The size of test dataframe",mobile_data_test.shape)
print("The size of train dataframe",train_data.shape)
print("The size of new dataframe",new_mobile_data.shape)

new_mobile_data.columns

data=new_mobile_data.copy()

#data_copy=data.copy()

"""# **Ranking**

**Ranking of each feature based on the column value and method dense is performed.**
"""

data=new_mobile_data.rank(ascending=False,axis=0,method="dense")
data.head()
data["price_range"].value_counts()
data=pd.concat([data, new_mobile_data], axis=1)
print(data.shape)
data.head(2)

#price range
data_price=data[["price_range","price_range"]]
price_rank=data_price.iloc[:,0:2]
price_rank.columns=["rank on price range","price_range"]
price_rank.drop_duplicates(["price_range"])

"""**Price rank of 0 has the lowest rank and 3 has the highest rank**"""

#battery power
data_battery=data[["battery_power","battery_power"]]
battery_rank=data_battery.iloc[:,0:2]
battery_rank.columns=["rank on battery","battery_power"]
battery_rank.drop_duplicates(["battery_power"]).sort_values(by="rank on battery")

#blue
data_blue=data[["blue","blue"]]
blue_rank=data_blue.iloc[:,0:2]
blue_rank.columns=["rank on bluetooth","bluetooth"]
blue_rank.drop_duplicates(["bluetooth"]).sort_values(by="rank on bluetooth")

#clock_speed
data_clock_speed=data[['clock_speed','clock_speed']]

clock_speed_rank=data_clock_speed.iloc[:,0:2]

clock_speed_rank.columns=["rank on clock_speed",'clock_speed']
clock_speed_rank.drop_duplicates(['clock_speed']).sort_values(by="rank on clock_speed")

#dual_sim
data_dual_sim=data[['dual_sim','dual_sim']]
dual_sim_rank=data_dual_sim.iloc[:,0:2]
dual_sim_rank.columns=["rank on dual_sim",'dual_sim']
dual_sim_rank.drop_duplicates(['dual_sim']).sort_values(by="rank on dual_sim")

#front cam
data_fc=data[['fc','fc']]
fc_rank=data_fc.iloc[:,0:2]
fc_rank.columns=["rank on fc",'fc']
fc_rank.drop_duplicates(['fc']).sort_values(by="rank on fc")

#four_g
data_four_g=data[['four_g','four_g']]
four_g_rank=data_four_g.iloc[:,0:2]
four_g_rank.columns=["rank on four_g",'four_g']
four_g_rank.drop_duplicates(['four_g']).sort_values(by="rank on four_g")

#internal memory
data_int_memory=data[['int_memory','int_memory']]
int_memory_rank=data_int_memory.iloc[:,0:2]
int_memory_rank.columns=["rank on int_memory",'int_memory']
int_memory_rank.drop_duplicates(['int_memory']).sort_values(by="rank on int_memory")

#mobile depth
data_m_dep=data[['m_dep','m_dep']]
m_dep_rank=data_m_dep.iloc[:,0:2]
m_dep_rank.columns=["rank on m_dep",'m_dep']
m_dep_rank.drop_duplicates(['m_dep']).sort_values(by="rank on m_dep")

#mobile weight
data_mobile_wt=data[['mobile_wt','mobile_wt']]
mobile_wt_rank=data_mobile_wt.iloc[:,0:2]
mobile_wt_rank.columns=["rank on mobile_wt",'mobile_wt']
mobile_wt_rank.drop_duplicates(['mobile_wt']).sort_values(by="rank on mobile_wt")

#n cores
data_n_cores=data[['n_cores','n_cores']]
n_cores_rank=data_n_cores.iloc[:,0:2]
n_cores_rank.columns=["rank on n_cores",'n_cores']
n_cores_rank.drop_duplicates(['n_cores']).sort_values(by="rank on n_cores")

#back camera
data_pc=data[['pc','pc']]
pc_rank=data_pc.iloc[:,0:2]
pc_rank.columns=["rank on pc",'pc']
pc_rank.drop_duplicates(['pc']).sort_values(by="rank on pc")

#pixel height
data_px_height=data[['px_height','px_height']]
px_height_rank=data_px_height.iloc[:,0:2]
px_height_rank.columns=["rank on px_height",'px_height']
px_height_rank.drop_duplicates(['px_height']).sort_values(by="rank on px_height")

#pixel weight
data_px_width=data[['px_width','px_width']]
px_width_rank=data_px_width.iloc[:,0:2]
px_width_rank.columns=["rank on px_width",'px_width']
px_width_rank.drop_duplicates(['px_width']).sort_values(by="rank on px_width")

#ram
data_ram=data[['ram','ram']]
ram_rank=data_ram.iloc[:,0:2]
ram_rank.columns=["rank on ram",'ram']
ram_rank.drop_duplicates(['ram']).sort_values(by="rank on ram")

#screen height
data_sc_h=data[['sc_h','sc_h']]
sc_h_rank=data_sc_h.iloc[:,0:2]
sc_h_rank.columns=["rank on sc_h",'sc_h']
sc_h_rank.drop_duplicates(['sc_h']).sort_values(by="rank on sc_h")

#screen width
data_sc_w=data[['sc_w','sc_w']]
sc_w_rank=data_sc_w.iloc[:,0:2]
sc_w_rank.columns=["rank on sc_w",'sc_w']
sc_w_rank.drop_duplicates(['sc_w']).sort_values(by="rank on sc_w")

#talk_time
data_talk_time=data[['talk_time','talk_time']]
talk_time_rank=data_talk_time.iloc[:,0:2]
talk_time_rank.columns=["rank on talk_time",'talk_time']
talk_time_rank.drop_duplicates(['talk_time']).sort_values(by="rank on talk_time")

#three_g
data_three_g=data[['three_g','three_g']]
three_g_rank=data_three_g.iloc[:,0:2]
three_g_rank.columns=["rank on three_g",'three_g']
three_g_rank.drop_duplicates(['three_g']).sort_values(by="rank on three_g")

#touch_screen
data_touch_screen=data[['touch_screen','touch_screen']]
touch_screen_rank=data_touch_screen.iloc[:,0:2]
touch_screen_rank.columns=["rank on touch_screen",'touch_screen']
touch_screen_rank.drop_duplicates(['touch_screen']).sort_values(by="rank on touch_screen")

#wifi
data_wifi=data[['wifi','wifi']]
wifi_rank=data_wifi.iloc[:,0:2]
wifi_rank.columns=["rank on wifi",'wifi']
wifi_rank.drop_duplicates(['wifi']).sort_values(by="rank on wifi")

""" **Ranking on Row**"""

train_data=mobile_data.copy()

#ranking the entire column of dataset
ranked_mobiledata_row= mobile_data.rank()
ranked_mobiledata_row.head(3)

"""It is difficult for us to interpret from this dataframe si we are going to deal with each columns of the dataset."""

#@title
#ranking the entire column of dataset
ranked_mobiledata_column= mobile_data.rank(axis=1)
ranked_mobiledata_column.head(3)

#@title
#rank based on price range
mobile_data["price_range_ranking_row"]=mobile_data["price_range"].rank(ascending=False)
mobile_data.sort_values("price_range",ascending=False).head(3)

#@title
type(mobile_data["price_range"])

#@title
#rank based on price range on column value
#rank based on price range
mobile_data["rank_based_on_price"]=mobile_data["price_range"].rank(ascending=False)
mobile_data.sort_values("price_range",ascending=False).head(3)

#@title

mobile_data[["price_range","rank_based_on_price"]].sort_values("price_range",ascending=False).head(3)

#@title
mobile_data[["price_range","rank_based_on_price"]].sort_values("price_range").head(3)

"""**Price range of 3 has the lowest ranking**

---


"""

#@title
#rank based on battery power
mobile_data["rank_based on_battery"]=mobile_data[["battery_power"]].rank()
mobile_data[["battery_power","rank_based on_battery"]].sort_values("battery_power",ascending=False).head(3)

#@title
mobile_data[["battery_power","rank_based on_battery"]].sort_values("battery_power").head(3)

"""The battery power 1998 has the highest rank. abd 501 has the lowest."""

#@title
#rank based on blue
mobile_data["rank_by_blue"]=mobile_data[["blue"]].rank()
mobile_data[["blue","rank_by_blue"]].sort_values("blue",ascending=False).head(3)

#@title
mobile_data[["blue","rank_by_blue"]].sort_values("blue").head(3)

"""bluetooth = yes has the highest rank and no lowest"""

#@title
#rank based on clock_speed
mobile_data["rank_by_clock_speed"]=mobile_data[["clock_speed"]].rank()
mobile_data[["clock_speed","rank_by_clock_speed"]].sort_values("clock_speed",ascending=False).head(3)

#@title
mobile_data[["clock_speed","rank_by_clock_speed"]].sort_values("clock_speed").head(3)

"""**Clock speed 3 has the highest rank and 0.5 has the lowest**"""

#@title
#rank based on dual_sim
mobile_data["rank_by_dualsim"]=mobile_data[["dual_sim"]].rank()
mobile_data[["dual_sim","rank_by_dualsim"]].sort_values("dual_sim",ascending=False).head(3)

#@title
mobile_data[["dual_sim","rank_by_dualsim"]].sort_values("dual_sim").head(3)

"""**Smartphone having dual sim has the highest ranking and which doesnt have one has the lowest**"""

#@title
#rank based on fc
mobile_data["rank_by_fc"]=mobile_data[["fc"]].rank()
mobile_data[["fc","rank_by_fc"]].sort_values("fc",ascending=False).head(3)

#@title
mobile_data[["fc","rank_by_fc"]].sort_values("fc").head(3)

"""**Smartphome with front camera pixel of 16 has the highest ranking and 0 has the lowest**"""

#@title
#rank based on 4G
mobile_data["rank_by_4G"]=mobile_data[["four_g"]].rank()
mobile_data[["four_g","rank_by_4G"]].sort_values("four_g",ascending=False).head(3)

#@title
mobile_data[["four_g","rank_by_4G"]].sort_values("four_g").head(3)

"""**Smartphone having four g facility as the highest ranking and those doesnt have the one has the lowest.**"""

#@title
#rank based on internal memory
mobile_data["rank_by_int_mem"]=mobile_data[["int_memory"]].rank()
mobile_data[["int_memory","rank_by_int_mem"]].sort_values("int_memory",ascending=False).head(3)

#@title
mobile_data[["int_memory","rank_by_int_mem"]].sort_values("int_memory").head(3)

"""**Internal memory 64 has the highest rank and 2 has the lowest.**"""

#@title
#rank based on m_depth
mobile_data["rank_by_m_dep"]=mobile_data[["m_dep"]].rank()
mobile_data[["m_dep","rank_by_m_dep"]].sort_values("m_dep",ascending=False).head(3)

#@title
mobile_data[["m_dep","rank_by_m_dep"]].sort_values("m_dep").head(3)

"""**m_depth of 1 has the highest and 0.1 has the lowest rank**"""

#@title
#rank based on mobile_weight
mobile_data["rank_by_mobile_wt"]=mobile_data[["mobile_wt"]].rank()
mobile_data[["mobile_wt","rank_by_mobile_wt"]].sort_values("mobile_wt",ascending=False).head(3)

#@title
mobile_data[["mobile_wt","rank_by_mobile_wt"]].sort_values("mobile_wt").head(3)

"""**Mobile weight of 200 has the highest rank and 80 has the lowest.**"""

#@title
#rank based on no:of cores
mobile_data["rank_by_n_cores"]=mobile_data[["n_cores"]].rank()
mobile_data[["n_cores","rank_by_n_cores"]].sort_values("n_cores",ascending=False).head(3)

#@title
mobile_data[["n_cores","rank_by_n_cores"]].sort_values("n_cores").head(3)

"""**Smartphone with 8 cores have the highest ranking and that of single core have the lowest.**"""

#@title
#rank based on pc
mobile_data["rank_by_pc"]=mobile_data[["pc"]].rank()
mobile_data[["pc","rank_by_pc"]].sort_values("pc",ascending=False).head(3)

#@title
mobile_data[["pc","rank_by_pc"]].sort_values("pc").head(3)

"""**pc value of 20 has the highest ranking and 0 has the lowest**


"""

#@title
#rank based on pixel_height
mobile_data["rank_by_px_height"]=mobile_data[["px_height"]].rank()
mobile_data[["px_height","rank_by_px_height"]].sort_values("px_height",ascending=False).head(3)

#@title
mobile_data[["px_height","rank_by_px_height"]].sort_values("px_height").head(3)

"""**pixel height of 0 has the lowest ranking and 1960 has the highest.**"""

#@title
#rank based on px_width
mobile_data["rank_by_px_width"]=mobile_data[["px_width"]].rank()
mobile_data[["px_width","rank_by_px_width"]].sort_values("px_width",ascending=False).head(3)

#@title
mobile_data[["px_width","rank_by_px_width"]].sort_values("px_width").head(3)

"""**pixel width of 1998 has the highest rankinG and 500 has the lowest.**"""

#@title
#rank based on ram
mobile_data["rank_by_ram"]=mobile_data[["ram"]].rank()
mobile_data[["ram","rank_by_ram"]].sort_values("ram",ascending=False).head(3)

#@title
mobile_data[["ram","rank_by_ram"]].sort_values("ram").head(3)

"""**RAM of 3998 has the highest ranking and 256 has the lowest.**"""

#@title
#rank based on screen height
mobile_data["rank_by_sc_h"]=mobile_data[["sc_h"]].rank()
mobile_data[["sc_h","rank_by_sc_h"]].sort_values("sc_h",ascending=False).head(3)

#@title
mobile_data[["sc_h","rank_by_sc_h"]].sort_values("sc_h").head(3)

"""**screen height of 19 has the highest ranking and 5 has the lowest ranking**"""

#@title
#rank based on sc_w
mobile_data["rank_by_sc_w"]=mobile_data[["sc_w"]].rank()
mobile_data[["sc_w","rank_by_sc_w"]].sort_values("sc_w",ascending=False).head(3)

#@title
mobile_data[["sc_w","rank_by_sc_w"]].sort_values("sc_w").head(3)

"""**Screen width of 18 has the highest ranking and 0 has the lowest**"""

#@title
#rank based on talk_time
mobile_data["rank_by_talk_time"]=mobile_data[["talk_time"]].rank()
mobile_data[["talk_time","rank_by_talk_time"]].sort_values("talk_time",ascending=False).head(3)

#@title
mobile_data[["talk_time","rank_by_talk_time"]].sort_values("talk_time").head(3)

"""**Talk time of 20 has the highest and 2 has the lowest ranking.**"""

#@title
#rank based on three g
mobile_data["rank_by_three_g"]=mobile_data[["three_g"]].rank()
mobile_data[["three_g","rank_by_three_g"]].sort_values("three_g",ascending=False).head(3)

#@title
mobile_data[["three_g","rank_by_three_g"]].sort_values("three_g").head(3)

"""**Devices with three g facility has the highest ranking and those that doesnt have one has the lowest.**"""

#@title
#rank based on touch_screen facility
mobile_data["rank_by_touch_screen"]=mobile_data[["touch_screen"]].rank()
mobile_data[["touch_screen","rank_by_touch_screen"]].sort_values("touch_screen",ascending=False).head(3)

#@title
mobile_data[["touch_screen","rank_by_touch_screen"]].sort_values("touch_screen").head(3)

"""**Devices with touchscreen facility has thehighest ranking and those doesnt have one has the lowest.**"""

#@title
#rank based on wifi
mobile_data["rank_by_wifi"]=mobile_data[["wifi"]].rank()
mobile_data[["wifi","rank_by_wifi"]].sort_values("wifi",ascending=False).head(3)

#@title
mobile_data[["wifi","rank_by_wifi"]].sort_values("wifi").head(3)

"""**Devices that have wi fi facility has the highest ranking and those that doesnt have one has the lowest.**"""

#@title
mobile_data.columns

#@title
# ranking based on each column data
mobile_data_1=mobile_data.iloc[:,0:21]

rank_data_column=mobile_data_1.rank(axis=1,ascending=False,method='max')

rank_data_column.sort_values(ascending=False,by="price_range").head()

#@title
rank_data_column["price_range"].value_counts()

"""**Finding feature importance**"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
data=pd.read_csv("/content/MobileTrain.csv")
X = data.iloc[:,0:20]  #independent columns
y = data.iloc[:,-1]  
y_new=y = data.iloc[:,-1]   #target column i.e price range
#apply SelectKBest class to extract top 10 best features
bestfeatures = SelectKBest(score_func=chi2, k=10)
fit = bestfeatures.fit(X,y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)
#concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score']  #naming the dataframe columns
print(featureScores.nlargest(10,'Score'))  #print 10 best features

new_x=data[["ram","px_height","battery_power","px_width","mobile_wt","int_memory","sc_w","talk_time","fc","sc_h"]]

new_x.head(2)

new_x_1=new_x.copy()
#standard scaling
from sklearn.preprocessing import StandardScaler
scalar=StandardScaler()
new_x=scalar.fit_transform(new_x)
new_x=pd.DataFrame(new_x,columns=new_x_1.columns)
new_x.head(3)

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(new_x,y_new,random_state=0,test_size=0.25)

pip install CatBoost

from catboost import CatBoostClassifier
cat=CatBoostClassifier()
cat.fit(X_train,Y_train)
Y_p_cat=cat.predict(X_test)

#testing score
acc_cat_new_1=accuracy_score(Y_test,Y_p_cat)
print("Test Accuracy of Catboost classifier after feature reduction is : ",acc_cat_new_1 )
#confusion matrix
con_cat_new=confusion_matrix(Y_test,Y_p_cat)
print("\n The confusion matrix of Catboost classifier after feature reduction is \n\n",con_cat_new)
#classification report
print("\n The classification report of Catboost classifier after feature reduction is : \n")
print(classification_report(Y_test, Y_p_cat))

acc_cat

acc_cat_new_1

increased_acc=round(((acc_cat_new-acc_cat)/acc_cat_new)*100,2)
print("After doing feature reduction the accuracy of the cat boost is increased by {} %".format(increased_acc))

"""So we can reduced the fetures number to get improved accuracy."""